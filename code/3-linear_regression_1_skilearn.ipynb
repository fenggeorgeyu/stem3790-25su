{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49ecd726",
   "metadata": {},
   "source": [
    "*This notebook contains an excerpt from the [Python Data Science Handbook](http://shop.oreilly.com/product/0636920034919.do) by Jake VanderPlas; the content is available [on GitHub](https://github.com/jakevdp/PythonDataScienceHandbook).*\n",
    "\n",
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6351dc4e",
   "metadata": {},
   "source": [
    "Linear regression is a basic predictive modeling method which use a simple line to \"best\" fit a given set of data.\n",
    "\n",
    "![Linear Regression Example](https://upload.wikimedia.org/wikipedia/commons/3/3a/Linear_regression.svg) (from wikipedia)\n",
    "\n",
    "In this tutorial we will start with a quick intuitive walk-through of the mathematics behind this well-known problem, before seeing how before moving on to see how linear models can be generalized to account for more complicated patterns in data.\n",
    "\n",
    "We begin with the standard imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ab53e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c908f3f",
   "metadata": {},
   "source": [
    "## Simple Linear Regression\n",
    "\n",
    "We will start with the most familiar linear regression, a straight-line fit to data.\n",
    "A straight-line fit is a model of the form\n",
    "$$\n",
    "y = ax + b\n",
    "$$\n",
    "where $a$ is commonly known as the *slope*, and $b$ is commonly known as the *intercept*.\n",
    "\n",
    "Consider the following data, which is scattered about a line with a slope of 2 and an intercept of -5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce7c04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1)\n",
    "x = 10 * rng.rand(50)\n",
    "y = 2 * x - 5 + rng.randn(50)\n",
    "plt.scatter(x, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f630630",
   "metadata": {},
   "source": [
    "We can use Scikit-Learn's ``LinearRegression`` estimator to fit this data and construct the best-fit line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82d38ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression(fit_intercept=True)\n",
    "\n",
    "model.fit(x[:, np.newaxis], y)\n",
    "\n",
    "xfit = np.linspace(0, 10, 1000)\n",
    "yfit = model.predict(xfit[:, np.newaxis])\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.plot(xfit, yfit);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e809d87",
   "metadata": {},
   "source": [
    "The slope and intercept of the data are contained in the model's fit parameters, which in Scikit-Learn are always marked by a trailing underscore.\n",
    "Here the relevant parameters are ``coef_`` and ``intercept_``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1796c7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model slope:    \", model.coef_[0])\n",
    "print(\"Model intercept:\", model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2464b692",
   "metadata": {},
   "source": [
    "The predicting linear model is:\n",
    "\n",
    "$$y=2.027208810360696 x-4.9985770855532055$$\n",
    "\n",
    "We see that the results are very close to the inputs, as we might hope."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea7ff34",
   "metadata": {},
   "source": [
    "The ``LinearRegression`` estimator is much more capable than this, however—in addition to simple straight-line fits, it can also handle multidimensional linear models of the form\n",
    "$$\n",
    "y = a_0 + a_1 x_1 + a_2 x_2 + \\cdots\n",
    "$$\n",
    "where there are multiple $x$ values.\n",
    "Geometrically, this is akin to fitting a plane to points in three dimensions, or fitting a hyper-plane to points in higher dimensions.\n",
    "\n",
    "The multidimensional nature of such regressions makes them more difficult to visualize, but we can see one of these fits in action by building some example data, using NumPy's matrix multiplication operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86f1208",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1)\n",
    "X = 10 * rng.rand(100, 3)\n",
    "y = 0.5 + np.dot(X, [1.5, -2., 1.])\n",
    "\n",
    "model.fit(X, y)\n",
    "print(model.intercept_)\n",
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12a5292",
   "metadata": {},
   "source": [
    "Here the $y$ data is constructed from three random $x$ values, and the linear regression recovers the coefficients used to construct the data.\n",
    "\n",
    "In this way, we can use the single ``LinearRegression`` estimator to fit lines, planes, or hyperplanes to our data.\n",
    "It still appears that this approach would be limited to strictly linear relationships between variables, but it turns out we can relax this as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e07d80f",
   "metadata": {},
   "source": [
    "## Basis Function Regression\n",
    "\n",
    "One trick you can use to adapt linear regression to nonlinear relationships between variables is to transform the data according to *basis functions*.\n",
    "We have seen one version of this before, in the ``PolynomialRegression`` pipeline used in [Hyperparameters and Model Validation](05.03-Hyperparameters-and-Model-Validation.ipynb) and [Feature Engineering](05.04-Feature-Engineering.ipynb).\n",
    "The idea is to take our multidimensional linear model:\n",
    "$$\n",
    "y = a_0 + a_1 x_1 + a_2 x_2 + a_3 x_3 + \\cdots\n",
    "$$\n",
    "and build the $x_1, x_2, x_3,$ and so on, from our single-dimensional input $x$.\n",
    "That is, we let $x_n = f_n(x)$, where $f_n()$ is some function that transforms our data.\n",
    "\n",
    "For example, if $f_n(x) = x^n$, our model becomes a polynomial regression:\n",
    "$$\n",
    "y = a_0 + a_1 x + a_2 x^2 + a_3 x^3 + \\cdots\n",
    "$$\n",
    "Notice that this is *still a linear model*—the linearity refers to the fact that the coefficients $a_n$ never multiply or divide each other.\n",
    "What we have effectively done is taken our one-dimensional $x$ values and projected them into a higher dimension, so that a linear fit can fit more complicated relationships between $x$ and $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bf6519",
   "metadata": {},
   "source": [
    "### Polynomial basis functions\n",
    "\n",
    "This polynomial projection is useful enough that it is built into Scikit-Learn, using the ``PolynomialFeatures`` transformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a051e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "x2 = np.array([2, 3, 4])\n",
    "poly = PolynomialFeatures(3, include_bias=False)\n",
    "poly.fit_transform(x2[:, None])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342a32b2",
   "metadata": {},
   "source": [
    "We see here that the transformer has converted our one-dimensional array into a three-dimensional array by taking the exponent of each value.\n",
    "This new, higher-dimensional data representation can then be plugged into a linear regression.\n",
    "\n",
    "As we saw in [Feature Engineering](05.04-Feature-Engineering.ipynb), the cleanest way to accomplish this is to use a pipeline.\n",
    "Let's make a 7th-degree polynomial model in this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3fbc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "poly_model = make_pipeline(PolynomialFeatures(7),\n",
    "                           LinearRegression())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dfda35",
   "metadata": {},
   "source": [
    "With this transform in place, we can use the linear model to fit much more complicated relationships between $x$ and $y$. \n",
    "For example, here is a sine wave with noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60ef4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1)\n",
    "x3 = 10 * rng.rand(50)\n",
    "y3 = np.sin(x) + 0.1 * rng.randn(50)\n",
    "\n",
    "poly_model.fit(x3[:, np.newaxis], y3)\n",
    "yfit = poly_model.predict(xfit[:, np.newaxis])\n",
    "\n",
    "plt.scatter(x3, y3)\n",
    "plt.plot(xfit, yfit);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82cf021",
   "metadata": {},
   "source": [
    "Our linear model, through the use of 7th-order polynomial basis functions, can provide an excellent fit to this non-linear data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e219e6d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
