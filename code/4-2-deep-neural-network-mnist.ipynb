{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More PyTorch tutorials can be found at [https://pytorch.org/tutorials/](https://pytorch.org/tutorials/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import pytorch libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim, cuda\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "use_gpu=True\n",
    "if cuda.is_available():\n",
    "    # check if GPU is available\n",
    "    print(cuda.get_device_properties(0))\n",
    "\n",
    "# for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps to create a Neural Networks with Pytorch\n",
    "\n",
    "1. Define dataset, data loader and transformations\n",
    "1. Define a Neural Network (forward propagation)\n",
    "1. Define optimizer and criterion\n",
    "1. Train the model (back propagation)\n",
    "1. Evaluate the model\n",
    "1. Make prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Define dataset, data loader and transformation\n",
    "\n",
    "<!-- PyTorch gives use the freedom to pretty much do anything with the Dataset class so long as you override two of the subclass functions:\n",
    "* the __len__ function which returns the size of the dataset, and\n",
    "* the __getitem__ function which returns a sample from the dataset given an index.\n",
    "\n",
    "However, in our case we can simply construct a TensorDataset with two items: the feature data and the target where the feature data is the matrix of pixel 1 - pixel 784 and the target is the digit of the image. -->\n",
    "\n",
    "\n",
    "While the Dataset class is a nice way of containing data systematically, it seems that in a training loop, we will need to index or slice the dataset's samples list. This is no better than what we would do for a typical list or NumPy matrix. Rather than going down that route, PyTorch supplies another utility function called the DataLoader which acts as a data feeder for a Dataset object.\n",
    "\n",
    "\n",
    "In order to construct the data loader we will need to provide two parameters: **batch_size** which indicates how many samples we want to use to train  the model in a batch, and **shuflle**, suggesting if we want to shuffle the data before sending it to the network. \n",
    "\n",
    "Typically we would want to set batch_size as $2^n$ e.g. 128, 256, 512, and set `shuffle` as `True` for traning data and `False` for validation and test data (you can take a moment to think why?)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing transforming functions\n",
    "# data_tf=transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.5],[0.5])])\n",
    "data_tf=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "# download datasets\n",
    "train_dataset = datasets.MNIST(root='../data', train=True, transform=data_tf, download=True)\n",
    "test_dataset = datasets.MNIST(root='../data', train=False, transform=data_tf, download=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader\n",
    "batch_size=64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display sample pictures from the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "<!-- row_num=8 # if batch_size=64, plot matrix as 8x8\n",
    "column_num=int(batch_size/row_num)\n",
    "\n",
    "#---create a grid figure----\n",
    "fig, axes = plt.subplots(row_num, column_num, figsize=(8, 8),\n",
    "                         subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                         gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "\n",
    "#---iterate and plot figures---\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(test_dataset.data[i], cmap='Greys', interpolation='nearest')\n",
    "    ax.text(0.1, 0.1, test_dataset.targets[i].numpy(), transform=ax.transAxes, color='green')\n",
    " -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "data=iter(sample_loader)\n",
    "samples,sample_labels=next(data)\n",
    "\n",
    "row_num=8 # if batch_size=64, plot matrix as 8x8\n",
    "column_num=int(batch_size/row_num)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(row_num, column_num, figsize=(8, 8),\n",
    "                         subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                         gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(samples[i][0], cmap='Greys', interpolation='nearest')\n",
    "    ax.text(0.1, 0.1, sample_labels[i].numpy(), transform=ax.transAxes, color='green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Define a Deep Neural Networks\n",
    "\n",
    "<!-- ![nn](images/neural_network_example.png)\n",
    "\n",
    "The neural network architectures in Pytorch can be defined in a class which inherits the properties from the base class from **nn** package called Module. This inheritance from the nn.Module class allows us to implement, access, and call a number of methods easily. We can define all the layers inside the constructor of the class, and the forward propagation steps inside the forward function.\n",
    "\n",
    "We will define a simple Multilayer Perceptron with the following architecture:\n",
    "\n",
    "* Input layer\n",
    "```Python\n",
    "nn.Linear(28 * 28, 512)\n",
    "```\n",
    "    * Layer type: nn.Linear(), which refers to a fully connection layer\n",
    "    * Input size: 28*28, corresponding to the size of input data.\n",
    "    * Output size: 512, the number of \"neurons\".\n",
    "    \n",
    "* Hidden layer\n",
    "```\n",
    "nn.Linear(512, 256)\n",
    "```\n",
    "\n",
    "    * Layer type: nn.Linear()\n",
    "    * Input size: 512, output size of the previous layer(input layer).\n",
    "    * Output size: 256, the number of \"neurons\" in this layer.\n",
    "    \n",
    "* Output layer\n",
    "```\n",
    "nn.Linear(256, 10)\n",
    "```\n",
    "\n",
    "    * Layer type: nn.Linear()\n",
    "    * Input size: 256, output size of the previous layer(hidden layer).\n",
    "    * Output size: 10, the number of classes we need to predict.\n",
    "\n",
    "* Activation functions\n",
    "Each linear layer's output needs to go through an activation function to \"activate\" it. We will get started with **F.sigmoid()** but can try F.relu() or others later.\n",
    "\n",
    "The best practice is to name each layer and initialize them in the **__init__()** function as named building blocks and put the building blocks together in the **forward()** function which defines how the data actually flows in the network. In our case, each layer simply takes the output of the previous layer and perform transformations the generate outputs in sequence. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LeNet](images/lenet.png)\n",
    "LeNet. Original image published in [LeCun et al., 1998]\n",
    "\n",
    "[LeCun et al., 1998]: https://ieeexplore.ieee.org/document/726791/\n",
    "\n",
    "A **convolution neural network (CNN)** performs the operation of **convolution** which adds elements of each image pixel to its local neighbor, weighted by a matrix or a small matrix, which helps to extract local features (e.g. sharpness, blurness, and edge) in an image. A major difference between a convolution layer and a **fully connected (FC)** layer is each element or neuron in a convolution layer is only connected to its neighbors; however, in a FC layer, each element is connected to all elements from the previous layer.\n",
    "\n",
    "![Convolution](images/convolution-example.png)\n",
    "\n",
    "Convolution example. Picture from https://towardsdatascience.com/simple-introduction-to-convolutional-neural-networks-cdf8d3077bac.\n",
    "\n",
    "Another important operation is **pooling**. Pooling is used to reduce the dimension of data (tenor) in the neural network. A frequently used pooling method is called \"max pool\" which first divide each tensor into smaller subsets, and get the maximum value of each subset that is used to form a new tensor with smaller dimension.\n",
    "\n",
    "![Maxpool](images/maxpool-example.png)\n",
    "\n",
    "Max pool example. Picture from https://analyticsindiamag.com/max-pooling-in-convolutional-neural-network-and-its-features/\n",
    "\n",
    "A **dropout** layer randomly \"drops\" a network element or set its weight to zero. The purpose is to avoid generating a too \"complicated\" network which tends to overfit the data. Overfiting the data will decrease the generalization ability of the neural network meaning the model will perform poorly given unsean validation or test data.\n",
    "\n",
    "A **deep neural network (DNN)**, such as CNN, is a composition of mulitple types of layers such as convlution layers, pooling layers, dropout layers, fully connected layers, etc. The final output layer can be a `softmax` layer which yeilds a result or label with the maximum prediciton probability. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNExample(nn.Module):\n",
    "    \"\"\"\n",
    "    from: https://pytorch.org/tutorials/recipes/recipes/defining_a_neural_network.html\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(CNNExample, self).__init__()\n",
    "        \n",
    "        # First 2D convolutional layer, taking in 1 input channel (grayscale image),\n",
    "        # outputting 32 convolutional features, with a square kernel size of 3\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        # Second 2D convolutional layer, taking in the 32 input layers,\n",
    "        # outputting 64 convolutional features, with a square kernel size of 3\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "\n",
    "        # Designed to ensure that adjacent pixels are either all 0s or all active\n",
    "        # with an input probability\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "\n",
    "        # First fully connected layer\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        # Second fully connected layer that outputs our 10 labels\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x) # conv layer 1\n",
    "        x = F.relu(x) # activation function\n",
    "        x = self.conv2(x) # conv layer 2\n",
    "        x = F.max_pool2d(x, 2) # pool layer\n",
    "        x = self.dropout1(x) # dropout layer 1\n",
    "        x = torch.flatten(x, 1) # flatten layer \n",
    "        x = self.fc1(x) # full connection layer 1\n",
    "        x = F.relu(x) # activation function\n",
    "        x = self.dropout2(x) # dropout layer 2\n",
    "        x = self.fc2(x) # full connection layer 2\n",
    "        output = F.log_softmax(x, dim=1) # output use softmax function\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model for training\n",
    "The following code defines a neural network. It will be used in later coding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "# define model\n",
    "model = BatchNet(28*28, 300, 100, 10)\n",
    "if cuda.is_available(): \n",
    "    #if GPU is available\n",
    "    model=model.cuda()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Define optimizer and criterion\n",
    "\n",
    "Optimizer is used to perform the gradient descent process. There are several optimizers avaialble such as SGD( Stochastic Gradient Descent), Adam, Adagrad, etc. The tricky part is how to set the right size of learning rate (`learning_rate`) which could have a huge impact on the final result. Learning rate is step size when training the neural network. A larger step size trains quicker but may \"miss\" the optimal solution. For example, let's simply use 0.1 as the starting point.\n",
    "\n",
    "Criterion will be used to calculate the cost (or loss) so we can use the cost to do back propagation and update the weights we want to train. In our case, we will use `nn.CrossEntropyLoss()` since we are working on a multiclassfication problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "#---optimizer for training the neural network---\n",
    "learning_rate=1e-1\n",
    "\n",
    "# use Adam optimizer\n",
    "optimizer=optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#---criterion is the cost or error function---\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 train the model (back propagation)\n",
    "\n",
    "Training the model is an iterative process which contains many epoches. For each epoch we will repeatly load batches of data, perform forward propagation, calculate cost, perform back propagation using the optimizer.\n",
    "\n",
    "**Epoch** is how many times one wants to train the neural network. Each epoch will load and train all the training data through the neural network. After each epoch, the cost (error) function tends to get lower errors. Larger number of epoch means more rounds of training and may further lower the error of training but will take longer time. However, you may notice in the experiment that a very large number of epoch may not be necessary if the training error is \"acceptable\" after a lower number of rounds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########training############\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss=0\n",
    "        for data in train_loader:\n",
    "            img, label = data\n",
    "            #img = img.view(img.size(0),-1)\n",
    "            if use_gpu and cuda.is_available(): #if use_gpu switch is on\n",
    "                img=Variable(img).cuda()\n",
    "                label=Variable(label).cuda()\n",
    "            else:\n",
    "                img=Variable(img)\n",
    "                label=Variable(label)\n",
    "            ##forward training\n",
    "            out = model(img)\n",
    "            loss = criterion(out, label)\n",
    "            #loss=F.nll_loss(out,label)\n",
    "            train_loss += loss.data*label.size(0)\n",
    "            #print(loss.data)\n",
    "            #print(label.size(0))\n",
    "            train_loss+=loss.item()\n",
    "            # backward propagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        #scheduler.step()\n",
    "        if (epoch+1)%(num_epochs/10) ==0:\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}, eval_loss={train_loss/(len(test_dataset))}')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Test the model\n",
    "\n",
    "We will use the trained model to make predictions on the validation dataset and compare the predictions against the actual targets. Dataloader will be used to iterate the validation dataset as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########testing##########\n",
    "def test_model(model, criterion, test_loader):\n",
    "    eval_loss=0\n",
    "    eval_acc=0\n",
    "    #use evaluation model\n",
    "    model.eval()\n",
    "    for data in test_loader:\n",
    "        img, label = data\n",
    "        #img = img.view(img.size(0),-1)\n",
    "        if use_gpu and cuda.is_available(): #if use_gpu switch is on\n",
    "            img=Variable(img).cuda()\n",
    "            label=Variable(label).cuda()\n",
    "        else:\n",
    "            img=Variable(img)\n",
    "            label=Variable(label)\n",
    "        out = model(img)\n",
    "        loss = criterion(out, label)\n",
    "        eval_loss += loss.data*label.size(0)\n",
    "        _, pred = torch.max(out, 1) #???\n",
    "        num_correct = (pred==label).sum()\n",
    "        eval_acc += num_correct.data\n",
    "        #print('pred: {}, label: {}, num_correct: {}'.format(pred, label, num_correct))\n",
    "    loss=eval_loss/(len(test_dataset))\n",
    "    accuracy=eval_acc*1.0/(len(test_dataset))\n",
    "    print(f'Test Loss: {loss}, Acc: {accuracy}')\n",
    "    return loss, accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Putting Together and Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#gpu switch on/off\n",
    "use_gpu=True\n",
    "\n",
    "# # define model\n",
    "# model=CNN()\n",
    "model=CNNExample()\n",
    "# model = CNN2()\n",
    "# print(model)\n",
    "\n",
    "if use_gpu and cuda.is_available():\n",
    "    print(f'using GPU: {cuda.get_device_properties(0)}')\n",
    "    model=model.cuda()\n",
    "else:\n",
    "    print('using CPU')\n",
    "    model=model.cpu()\n",
    "    \n",
    "# learning rate\n",
    "learning_rate=1e-1\n",
    "#learning_rate=1\n",
    "    \n",
    "#---optimizer for training the neural network---\n",
    "# use Adadelta optimizer: https://pytorch.org/docs/stable/generated/torch.optim.Adadelta.html\n",
    "optimizer=optim.Adadelta(model.parameters(), lr=learning_rate)\n",
    "##---uncomment the following lines to try out different optimizers---\n",
    "#optimizer=optim.Adagrad(model.parameters(), lr=learning_rate)\n",
    "#optimizer=optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#optimizer=optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "#---criterion is the cost or error function---\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "#criterion=nn.NLLLoss()\n",
    "\n",
    "# #---scheduler---\n",
    "# scheduler = StepLR(optimizer, step_size=1, gamma=0.7) # need by NLLLoss\n",
    "\n",
    "# set number of epochs\n",
    "num_epochs=10\n",
    "\n",
    "# train model\n",
    "model = train_model(model, train_loader, criterion, optimizer, num_epochs)\n",
    "loss, accuracy=test_model(model, criterion, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Sample results:\n",
    "\n",
    "\n",
    "* Macbook Pro M1 CPU:\n",
    "\n",
    "learning rate|number of epocs|criterion|optimizer\n",
    "---|---|---|---\n",
    "0.1|20|cross entropy|Adadelta\n",
    "\n",
    "\n",
    "    using CPU\n",
    "    Epoch 2/20, eval_loss=0.7517828941345215\n",
    "    Epoch 4/20, eval_loss=0.48508456349372864\n",
    "    Epoch 6/20, eval_loss=0.3670792579650879\n",
    "    Epoch 8/20, eval_loss=0.31273218989372253\n",
    "    Epoch 10/20, eval_loss=0.2832340598106384\n",
    "    Epoch 12/20, eval_loss=0.23937365412712097\n",
    "    Epoch 14/20, eval_loss=0.2233337163925171\n",
    "    Epoch 16/20, eval_loss=0.2035352885723114\n",
    "    Epoch 18/20, eval_loss=0.1921214461326599\n",
    "    Epoch 20/20, eval_loss=0.179552361369133\n",
    "    Test Loss: 0.03176043927669525, Acc: 0.9908999800682068\n",
    "    CPU times: user 18min 9s, sys: 4min 38s, total: 22min 48s\n",
    "    Wall time: 7min 2s\n",
    "\n",
    "---\n",
    "\n",
    "* STEM Cloud GPU-PC3 CPU:\n",
    "\n",
    "<!-- learning rate: 0.1; number of epocs: 20; criterion: cross entropy; optimizer: Adadelta -->\n",
    "\n",
    "learning rate|number of epocs|criterion|optimizer\n",
    "---|---|---|---\n",
    "0.1|20|cross entropy|Adadelta\n",
    "\n",
    "```\n",
    "using CPU\n",
    "Epoch 2/20, eval_loss=0.7576344013214111\n",
    "Epoch 4/20, eval_loss=0.44593003392219543\n",
    "Epoch 6/20, eval_loss=0.3518705666065216\n",
    "Epoch 8/20, eval_loss=0.30115020275115967\n",
    "Epoch 10/20, eval_loss=0.2703513503074646\n",
    "Epoch 12/20, eval_loss=0.2431371510028839\n",
    "Epoch 14/20, eval_loss=0.2065913826227188\n",
    "Epoch 16/20, eval_loss=0.194166898727417\n",
    "Epoch 18/20, eval_loss=0.19129399955272675\n",
    "Epoch 20/20, eval_loss=0.17096801102161407\n",
    "Test Loss: 0.04144658148288727, Acc: 0.9889000058174133\n",
    "CPU times: user 29min 8s, sys: 1min 9s, total: 30min 18s\n",
    "Wall time: 8min 33s\n",
    "```\n",
    "\n",
    "learning rate|number of epocs|criterion|optimizer\n",
    "---|---|---|---\n",
    "0.1|40|cross entropy|Adadelta\n",
    "\n",
    "```\n",
    "using CPU\n",
    "Epoch 4/40, eval_loss=0.4648208022117615\n",
    "Epoch 8/40, eval_loss=0.3049250841140747\n",
    "Epoch 12/40, eval_loss=0.24013623595237732\n",
    "Epoch 16/40, eval_loss=0.205618217587471\n",
    "Epoch 20/40, eval_loss=0.16693182289600372\n",
    "Epoch 24/40, eval_loss=0.14103581011295319\n",
    "Epoch 28/40, eval_loss=0.12344837933778763\n",
    "Epoch 32/40, eval_loss=0.103425033390522\n",
    "Epoch 36/40, eval_loss=0.09953168779611588\n",
    "Epoch 40/40, eval_loss=0.09446790814399719\n",
    "Test Loss: 0.03882128745317459, Acc: 0.9900000095367432\n",
    "CPU times: user 57min 14s, sys: 3.23 s, total: 57min 17s\n",
    "Wall time: 16min 15s\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "* STEM Cloud GPU-PC3 GPU:\n",
    "\n",
    "<!-- learning rate: 0.1; number of epocs: 20; criterion: cross entropy; optimizer: Adadelta -->\n",
    "\n",
    "learning rate|number of epocs|criterion|optimizer\n",
    "---|---|---|---\n",
    "0.1|20|cross entropy|Adadelta\n",
    "\n",
    "```\n",
    "using GPU: _CudaDeviceProperties(name='Quadro RTX 4000', major=7, minor=5, total_memory=7979MB, multi_processor_count=36)\n",
    "Epoch 2/20, eval_loss=0.6966244578361511\n",
    "Epoch 4/20, eval_loss=0.4318397343158722\n",
    "Epoch 6/20, eval_loss=0.33928409218788147\n",
    "Epoch 8/20, eval_loss=0.2915371060371399\n",
    "Epoch 10/20, eval_loss=0.25787410140037537\n",
    "Epoch 12/20, eval_loss=0.2114899903535843\n",
    "Epoch 14/20, eval_loss=0.20517906546592712\n",
    "Epoch 16/20, eval_loss=0.18708714842796326\n",
    "Epoch 18/20, eval_loss=0.1714428961277008\n",
    "Epoch 20/20, eval_loss=0.16019845008850098\n",
    "Test Loss: 0.031663019210100174, Acc: 0.989799976348877\n",
    "CPU times: user 2min 36s, sys: 760 ms, total: 2min 37s\n",
    "Wall time: 2min 37s\n",
    "```\n",
    "\n",
    "<!-- learning rate: 0.1; number of epocs: 40; criterion: cross entropy; optimizer: Adadelta -->\n",
    "\n",
    "learning rate|number of epocs|criterion|optimizer\n",
    "---|---|---|---\n",
    "0.1|40|cross entropy|Adadelta\n",
    "\n",
    "```\n",
    "using GPU: _CudaDeviceProperties(name='Quadro RTX 4000', major=7, minor=5, total_memory=7979MB, multi_processor_count=36)\n",
    "Epoch 4/40, eval_loss=0.4653308093547821\n",
    "Epoch 8/40, eval_loss=0.3177127242088318\n",
    "Epoch 12/40, eval_loss=0.23504582047462463\n",
    "Epoch 16/40, eval_loss=0.19485096633434296\n",
    "Epoch 20/40, eval_loss=0.18018969893455505\n",
    "Epoch 24/40, eval_loss=0.14676715433597565\n",
    "Epoch 28/40, eval_loss=0.13056157529354095\n",
    "Epoch 32/40, eval_loss=0.12738990783691406\n",
    "Epoch 36/40, eval_loss=0.10587641596794128\n",
    "Epoch 40/40, eval_loss=0.09343577921390533\n",
    "Test Loss: 0.038114871829748154, Acc: 0.9907999634742737\n",
    "CPU times: user 5min 9s, sys: 655 ms, total: 5min 10s\n",
    "Wall time: 5min 11s\n",
    "```\n",
    "\n",
    "<!-- * Google Colab CPU:\n",
    "    * learning rate: 0.1; number of epocs: 20; criterion: cross entropy; optimizer: Adam\n",
    " -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: make prediction\n",
    "\n",
    "This step is similar to the validation step except that we are not comparing the predictions as there's no ground truth of target to compare with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size=batch_size\n",
    "# let shuffle be True to load random images each time\n",
    "sample_loader = DataLoader(test_dataset, batch_size=sample_size, shuffle=True) \n",
    "data=iter(sample_loader)\n",
    "samples,sample_labels=next(data)\n",
    "\n",
    "row_num=8 # if batch_size=64, plot matrix as 8x8\n",
    "column_num=int(batch_size/row_num)\n",
    "\n",
    "\n",
    "device = torch.device('cpu')\n",
    "model.to(device) #send model to cpu\n",
    "\n",
    "output=model(samples)\n",
    "_, pred = torch.max(output, 1)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(row_num, column_num, figsize=(8, 8),\n",
    "                         subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                         gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "\n",
    "error = 0 # prediction error\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(samples[i][0], cmap='Greys', interpolation='nearest')\n",
    "    ax.text(0.1, 0.1, sample_labels[i].numpy(), transform=ax.transAxes, color='green')\n",
    "    if sample_labels[i].numpy()==pred[i].numpy():\n",
    "        color_pred='green'\n",
    "    else:\n",
    "        color_pred='red'\n",
    "        error+=1\n",
    "    ax.text(0.8, 0.1, pred[i].numpy(), transform=ax.transAxes, color=color_pred)\n",
    "\n",
    "accuracy_predict=(1-error*1.0/sample_size)*100\n",
    "print(f'Predition Errors: {error} of {sample_size} | Prediction accuracy: {accuracy_predict}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "```Python\n",
    "model.eval()\n",
    "sample_size=batch_size # take same size sample as batch size\n",
    "sample_list=np.random.randint(len(test_loader), size=(sample_size))\n",
    "# print(f'sample list: {sample_list}')\n",
    "\n",
    "row_num=8 # if batch_size=64, plot matrix as 8x8\n",
    "column_num=int(sample_size/row_num)\n",
    "\n",
    "image_list=[test_dataset.data[i] for i in sample_list]\n",
    "label_list=[test_dataset.targets[i].numpy() for i in sample_list]\n",
    "image_tensor_list=torch.stack(tmp,0)\n",
    "image_tensor_list=image_tensor_list.view(ttmp.size(0),-1).float()#reshape and transfer to float data for learning\n",
    "\n",
    "output = model(image_tensor_list)\n",
    "_, pred = torch.max(output, 1)\n",
    "\n",
    "print(label_list)\n",
    "print(pred)\n",
    "\n",
    "#---create a grid figure----\n",
    "fig, axes = plt.subplots(row_num, column_num, figsize=(8, 8),\n",
    "                         subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                         gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "\n",
    "#---iterate and plot figures---\n",
    "error=0\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    image = image_list[i]\n",
    "    label_original = label_list[i]\n",
    "    ax.imshow(image, cmap='Greys', interpolation='nearest') # show image\n",
    "    ax.text(0.1, 0.1, label_original, transform=ax.transAxes, color='green') # show correct label\n",
    "    # if prediction is correct the color is green; o.w. the predition label is red\n",
    "    if label_original==pred[i].numpy(): \n",
    "        color_pred='green'\n",
    "    else:\n",
    "        color_pred='red'\n",
    "        error+=1\n",
    "    ax.text(0.8, 0.1, pred[i].numpy(), transform=ax.transAxes, color=color_pred)\n",
    "\n",
    "accuracy_predict=1-error*1.0/sample_size\n",
    "print(f'Prediction accuracy: {accuracy_predict}')\n",
    "```\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font style=\"font-size:30px\" color=\"red\">Your Turn</font>\n",
    "\n",
    "1. Try different values for num_epochs.\n",
    "\n",
    "1. Try different values for batch_size such as 64, 128, 256, 512.\n",
    "\n",
    "1. Try different learning rate (0.0001, 0.001, 0.1). What is your observation of the evalation loss (`eval_loss`) in training?\n",
    "\n",
    "1. `relu()` was used as the activation function for the MLP model we implemented. Try other activation functions such as `sigmoid()` and `tanh()`. What is your observation in training? (You may want to refer to the [PyTorch Functional](https://pytorch.org/docs/stable/nn.functional.html) for more details.)\n",
    "    \n",
    "1. Try different optimizers such as `SGD` and `Adagrad`.  What is your observation in training? (Refer to [PyTorch Optimizer](https://pytorch.org/docs/stable/optim.html))\n",
    "\n",
    "1. Try to add/ remove hiden layers, as well as different number of neurons and report your validation results.  What is your observation in training?\n",
    "\n",
    "1. What can you think of if we want to improve the current model?\n",
    "\n",
    "\n",
    "<!-- # Assignments: -->\n",
    "\n",
    "\n",
    "<!-- 1. Can you implment the following functions for model training, evaluation and prediction so we can reuse them when we need to test different things afterwards without having to replicate the codes every time.\n",
    "\n",
    "```python\n",
    "\n",
    "def train_model(nn_model, train_loader, optimizer, criterion, n_epoch):\n",
    "    # YOUR IMPLEMENTATION\n",
    "    return nn_model\n",
    "\n",
    "def eval_model(nn_model, val_loader):\n",
    "    # YOUR IMPLEMENTATION\n",
    "    return calculated_accuracy\n",
    "\n",
    "\n",
    "def predict(nn_model, test_loader):\n",
    "    # YOUR IMPLEMENTATION\n",
    "    return predictions\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "2. F.sigmoid() was used as the activation function for the MLP model we implemented. Can you try other activation functions such as F.relu() and F.tanh()? You may want to refer to the [PyTorch Functional](https://pytorch.org/docs/stable/nn.functional.html) for more details. Use the train_model(), eval_model() functions you implmented so you don't have to repeat the same codes.\n",
    "\n",
    "3. Can you add a dropout layer between input and the hidden layer, and another one between the hidden layer and the output layer?\n",
    "\n",
    "4. Try to add/ remove hiden layers, as well as different number of neurons and report your validation results.\n",
    "\n",
    "5. Try different learning rate (0.0001, 0.001, 0.01, 0.1). What is your observation?\n",
    "\n",
    "6. Try different values for batch_size(64, 128, 256, 512).\n",
    "\n",
    "7. Try different values for n_epochs.\n",
    "\n",
    "8. What can you think of if we want to improve the current model?\n",
    " -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (Conda 2022.05) [python/3.9-2022.05]",
   "language": "python",
   "name": "python39_202205"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
