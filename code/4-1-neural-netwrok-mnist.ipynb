{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More PyTorch tutorials can be found at [https://pytorch.org/tutorials/](https://pytorch.org/tutorials/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import pytorch libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim, cuda\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import torch.nn.functional as F\n",
    "use_gpu=False\n",
    "if cuda.is_available():\n",
    "    # check if GPU is available\n",
    "    print(cuda.get_device_properties(0))\n",
    "\n",
    "# for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps to create a Neural Networks with Pytorch\n",
    "\n",
    "1. Define dataset, data loader and transformations\n",
    "1. Define a Neural Network (forward propagation)\n",
    "1. Define optimizer and criterion\n",
    "1. Train the model (back propagation)\n",
    "1. Evaluate the model\n",
    "1. Make prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Define dataset, data loader and transformation\n",
    "\n",
    "<!-- PyTorch gives use the freedom to pretty much do anything with the Dataset class so long as you override two of the subclass functions:\n",
    "* the __len__ function which returns the size of the dataset, and\n",
    "* the __getitem__ function which returns a sample from the dataset given an index.\n",
    "\n",
    "However, in our case we can simply construct a TensorDataset with two items: the feature data and the target where the feature data is the matrix of pixel 1 - pixel 784 and the target is the digit of the image. -->\n",
    "\n",
    "\n",
    "While the Dataset class is a nice way of containing data systematically, it seems that in a training loop, we will need to index or slice the dataset's samples list. This is no better than what we would do for a typical list or NumPy matrix. Rather than going down that route, PyTorch supplies another utility function called the DataLoader which acts as a data feeder for a Dataset object.\n",
    "\n",
    "\n",
    "In order to construct the data loader we will need to provide two parameters: **batch_size** which indicates how many samples we want to use to train  the model in a batch, and **shuflle**, suggesting if we want to shuffle the data before sending it to the network. \n",
    "\n",
    "Typically we would want to set batch_size as $2^n$ e.g. 128, 256, 512, and set `shuffle` as `True` for traning data and `False` for validation and test data (you can take a moment to think why?)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing transforming functions\n",
    "data_tf=transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.5],[0.5])])\n",
    "\n",
    "# download datasets\n",
    "train_dataset = datasets.MNIST(root='../data', train=True, transform=data_tf, download=True)\n",
    "test_dataset = datasets.MNIST(root='../data', train=False, transform=data_tf, download=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader\n",
    "batch_size=64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display sample pictures from the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "<!-- row_num=8 # if batch_size=64, plot matrix as 8x8\n",
    "column_num=int(batch_size/row_num)\n",
    "\n",
    "#---create a grid figure----\n",
    "fig, axes = plt.subplots(row_num, column_num, figsize=(8, 8),\n",
    "                         subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                         gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "\n",
    "#---iterate and plot figures---\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(test_dataset.data[i], cmap='Greys', interpolation='nearest')\n",
    "    ax.text(0.1, 0.1, test_dataset.targets[i].numpy(), transform=ax.transAxes, color='green')\n",
    " -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "data=iter(sample_loader)\n",
    "samples,sample_labels=next(data)\n",
    "\n",
    "row_num=8 # if batch_size=64, plot matrix as 8x8\n",
    "column_num=int(batch_size/row_num)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(row_num, column_num, figsize=(8, 8),\n",
    "                         subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                         gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(samples[i][0], cmap='Greys', interpolation='nearest')\n",
    "    ax.text(0.1, 0.1, sample_labels[i].numpy(), transform=ax.transAxes, color='green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Define a Neural Networks\n",
    "\n",
    "![nn](images/neural_network_example.png)\n",
    "\n",
    "The neural network architectures in Pytorch can be defined in a class which inherits the properties from the base class from **nn** package called Module. This inheritance from the nn.Module class allows us to implement, access, and call a number of methods easily. We can define all the layers inside the constructor of the class, and the forward propagation steps inside the forward function.\n",
    "\n",
    "We will define a simple Multilayer Perceptron with the following architecture:\n",
    "\n",
    "* Input layer\n",
    "```Python\n",
    "nn.Linear(28 * 28, 512)\n",
    "```\n",
    "    * Layer type: nn.Linear(), which refers to a fully connection layer\n",
    "    * Input size: 28*28, corresponding to the size of input data.\n",
    "    * Output size: 512, the number of \"neurons\".\n",
    "    \n",
    "* Hidden layer\n",
    "```\n",
    "nn.Linear(512, 256)\n",
    "```\n",
    "\n",
    "    * Layer type: nn.Linear()\n",
    "    * Input size: 512, output size of the previous layer(input layer).\n",
    "    * Output size: 256, the number of \"neurons\" in this layer.\n",
    "    \n",
    "* Output layer\n",
    "```\n",
    "nn.Linear(256, 10)\n",
    "```\n",
    "\n",
    "    * Layer type: nn.Linear()\n",
    "    * Input size: 256, output size of the previous layer(hidden layer).\n",
    "    * Output size: 10, the number of classes we need to predict.\n",
    "\n",
    "* Activation functions\n",
    "Each linear layer's output needs to go through an activation function to \"activate\" it. We will get started with **F.sigmoid()** but can try F.relu() or others later.\n",
    "\n",
    "The best practice is to name each layer and initialize them in the **__init__()** function as named building blocks and put the building blocks together in the **forward()** function which defines how the data actually flows in the network. In our case, each layer simply takes the output of the previous layer and perform transformations the generate outputs in sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple 3-layer fully connected linear neural network (NN) or a Multi-Layer Perception (MLP) NN\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, n_hidden_1, n_hidden_2, out_dim):\n",
    "        super(simpleNet, self).__init__()\n",
    "        self.layer1=nn.Linear(in_dim, n_hidden_1)\n",
    "        self.layer2=nn.Linear(n_hidden_1, n_hidden_2)\n",
    "        self.layer3=nn.Linear(n_hidden_2, out_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x=self.layer1(x)\n",
    "        x=self.layer2(x)\n",
    "        x=self.layer3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network with activation\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, n_hidden_1, n_hidden_2, out_dim):\n",
    "        super(ActivationNet,self).__init__()\n",
    "        # input layer\n",
    "        self.layer1=nn.Sequential(nn.Linear(in_dim, n_hidden_1), nn.ReLU(True))\n",
    "        # hidden layer\n",
    "        self.layer2=nn.Sequential(nn.Linear(n_hidden_1, n_hidden_2), nn.ReLU(True))\n",
    "        # output layer\n",
    "        self.layer3=nn.Sequential(nn.Linear(n_hidden_2, out_dim))\n",
    "    \n",
    "    # connect layers\n",
    "    def forward(self, x):\n",
    "        x=self.layer1(x)\n",
    "        x=self.layer2(x)\n",
    "        x=self.layer3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network with normalization\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, n_hidden_1, n_hidden_2, out_dim):\n",
    "        super(BatchNet, self).__init__()\n",
    "        # input layer\n",
    "        self.layer1=nn.Sequential(nn.Linear(in_dim, n_hidden_1), nn.BatchNorm1d(n_hidden_1), nn.ReLU(True))\n",
    "        # hidden layer\n",
    "        self.layer2=nn.Sequential(nn.Linear(n_hidden_1, n_hidden_2), nn.BatchNorm1d(n_hidden_2), nn.ReLU(True))\n",
    "        # output layer\n",
    "        self.layer3=nn.Sequential(nn.Linear(n_hidden_2, out_dim))\n",
    "    \n",
    "    # connect layers\n",
    "    def forward(self, x):\n",
    "        x=self.layer1(x)\n",
    "        x=self.layer2(x)\n",
    "        x=self.layer3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model for training\n",
    "The following code defines a neural network. It will be used in later coding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "# define model\n",
    "model = BatchNet(28*28, 300, 100, 10)\n",
    "if cuda.is_available(): \n",
    "    #if GPU is available\n",
    "    model=model.cuda()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Define optimizer and criterion\n",
    "\n",
    "Optimizer is used to perform the gradient descent process. There are several optimizers avaialble such as SGD( Stochastic Gradient Descent), Adam, Adagrad, etc. The tricky part is how to set the right size of learning rate (`learning_rate`) which could have a huge impact on the final result. Learning rate is step size when training the neural network. A larger step size trains quicker but may \"miss\" the optimal solution. For example, let's simply use 0.1 as the starting point.\n",
    "\n",
    "Criterion will be used to calculate the cost (or loss) so we can use the cost to do back propagation and update the weights we want to train. In our case, we will use `nn.CrossEntropyLoss()` since we are working on a multiclassfication problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "#---optimizer for training the neural network---\n",
    "learning_rate=1e-1\n",
    "\n",
    "# use Adam optimizer\n",
    "optimizer=optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#---criterion is the cost or error function---\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 train the model (back propagation)\n",
    "\n",
    "Training the model is an iterative process which contains many epoches. For each epoch we will repeatly load batches of data, perform forward propagation, calculate cost, perform back propagation using the optimizer.\n",
    "\n",
    "**Epoch** is how many times one wants to train the neural network. Each epoch will load and train all the training data through the neural network. After each epoch, the cost (error) function tends to get lower errors. Larger number of epoch means more rounds of training and may further lower the error of training but will take longer time. However, you may notice in the experiment that a very large number of epoch may not be necessary if the training error is \"acceptable\" after a lower number of rounds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########training############\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss=0\n",
    "        for data in train_loader:\n",
    "            img, label = data\n",
    "            img = img.view(img.size(0),-1)\n",
    "            if use_gpu and cuda.is_available(): #if use_gpu switch is on\n",
    "                img=Variable(img).cuda()\n",
    "                label=Variable(label).cuda()\n",
    "            else:\n",
    "                img=Variable(img)\n",
    "                label=Variable(label)\n",
    "            # forward training\n",
    "            out = model(img)\n",
    "            loss = criterion(out, label)\n",
    "            train_loss += loss.data*label.size(0) \n",
    "            # backward propagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if (epoch+1)%(num_epochs/10) ==0:\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}, eval_loss={train_loss/(len(test_dataset))}')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Test the model\n",
    "\n",
    "We will use the trained model to make predictions on the validation dataset and compare the predictions against the actual targets. Dataloader will be used to iterate the validation dataset as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########testing##########\n",
    "def test_model(model, criterion, test_loader):\n",
    "    eval_loss=0\n",
    "    eval_acc=0\n",
    "    #use evaluation model\n",
    "    model.eval()\n",
    "    for data in test_loader:\n",
    "        img, label = data\n",
    "        img = img.view(img.size(0),-1)\n",
    "        if use_gpu and cuda.is_available(): #if use_gpu switch is on\n",
    "            img=Variable(img).cuda()\n",
    "            label=Variable(label).cuda()\n",
    "        else:\n",
    "            img=Variable(img)\n",
    "            label=Variable(label)\n",
    "        out = model(img)\n",
    "        loss = criterion(out, label)\n",
    "        eval_loss += loss.data*label.size(0)\n",
    "        _, pred = torch.max(out, 1) #???\n",
    "        num_correct = (pred==label).sum()\n",
    "        eval_acc += num_correct.data\n",
    "        #print('pred: {}, label: {}, num_correct: {}'.format(pred, label, num_correct))\n",
    "    loss=eval_loss/(len(test_dataset))\n",
    "    accuracy=eval_acc*1.0/(len(test_dataset))\n",
    "    print(f'Test Loss: {loss}, Acc: {accuracy}')\n",
    "    return loss, accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Putting Together and Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# # define model\n",
    "# #model = simpleNet(28*28, 300, 100, 10)\n",
    "model = BatchNet(28*28, 1000, 200, 10)\n",
    "use_gpu=True\n",
    "\n",
    "if use_gpu and cuda.is_available():\n",
    "    print(f'using GPU: {cuda.get_device_properties(0)}')\n",
    "    model=model.cuda()\n",
    "else:\n",
    "    print('using CPU')\n",
    "    model=model.cpu()\n",
    "    \n",
    "# learning rate\n",
    "learning_rate=1e-1\n",
    "    \n",
    "#---optimizer for training the neural network---\n",
    "# use Adam optimizer\n",
    "optimizer=optim.Adam(model.parameters(), lr=learning_rate)\n",
    "##---uncomment the following lines to try out different optimizers---\n",
    "#optimizer=optim.SGD(model.parameters(), lr=learning_rate)\n",
    "#optimizer=optim.Adagrad(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#---criterion is the cost or error function---\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "\n",
    "# set number of epochs\n",
    "num_epochs=20\n",
    "\n",
    "# train model\n",
    "model = train_model(model, train_loader, criterion, optimizer, num_epochs)\n",
    "loss, accuracy=test_model(model, criterion, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Sample results:\n",
    "\n",
    "* Macbook Pro M1 CPU:\n",
    "    * learning rate: 0.1; number of epocs: 20; criterion: cross entropy; optimizer: Adam\n",
    "\n",
    "```\n",
    "Epoch 2/20, eval_loss=1.0971201658248901\n",
    "Epoch 4/20, eval_loss=0.8115490078926086\n",
    "Epoch 6/20, eval_loss=0.6610875725746155\n",
    "Epoch 8/20, eval_loss=0.5784343481063843\n",
    "Epoch 10/20, eval_loss=0.5498111248016357\n",
    "Epoch 12/20, eval_loss=0.5375860333442688\n",
    "Epoch 14/20, eval_loss=0.4178693890571594\n",
    "Epoch 16/20, eval_loss=0.40172743797302246\n",
    "Epoch 18/20, eval_loss=0.4024063050746918\n",
    "Epoch 20/20, eval_loss=0.35567259788513184\n",
    "Test Loss: 0.1245042011141777, Acc: 0.9742000102996826\n",
    "CPU times: user 1min 6s, sys: 48.4 s, total: 1min 54s\n",
    "Wall time: 1min 13s\n",
    "```\n",
    "\n",
    "* Google Colab CPU:\n",
    "    * learning rate: 0.1; number of epocs: 20; criterion: cross entropy; optimizer: Adam\n",
    "\n",
    "```\n",
    "Epoch 2/20, eval_loss=1.1556735038757324\n",
    "Epoch 4/20, eval_loss=0.7978797554969788\n",
    "Epoch 6/20, eval_loss=0.6535661220550537\n",
    "Epoch 8/20, eval_loss=0.5536051988601685\n",
    "Epoch 10/20, eval_loss=0.5287511348724365\n",
    "Epoch 12/20, eval_loss=0.45398226380348206\n",
    "Epoch 14/20, eval_loss=0.4813585877418518\n",
    "Epoch 16/20, eval_loss=0.4048077464103699\n",
    "Epoch 18/20, eval_loss=0.35148611664772034\n",
    "Epoch 20/20, eval_loss=0.358542799949646\n",
    "Test Loss: 0.12928466498851776, Acc: 0.9761999845504761\n",
    "CPU times: user 5min 58s, sys: 1.64 s, total: 6min\n",
    "Wall time: 5min 59s\n",
    "```\n",
    "\n",
    "* STEM Cloud GPU-PC3 CPU:\n",
    "    * learning rate: 0.1; number of epocs: 20; criterion: cross entropy; optimizer: Adam\n",
    "\n",
    "```\n",
    "Epoch 2/20, eval_loss=1.0580682754516602\n",
    "Epoch 4/20, eval_loss=0.7820923924446106\n",
    "Epoch 6/20, eval_loss=0.651170551776886\n",
    "Epoch 8/20, eval_loss=0.5649536848068237\n",
    "Epoch 10/20, eval_loss=0.5339972972869873\n",
    "Epoch 12/20, eval_loss=0.47456058859825134\n",
    "Epoch 14/20, eval_loss=0.4599563479423523\n",
    "Epoch 16/20, eval_loss=0.3801591694355011\n",
    "Epoch 18/20, eval_loss=0.3814277648925781\n",
    "Epoch 20/20, eval_loss=0.3756769895553589\n",
    "Test Loss: 0.14153259992599487, Acc: 0.9740999937057495\n",
    "CPU times: user 7min 8s, sys: 656 ms, total: 7min 9s\n",
    "Wall time: 2min 47s\n",
    "```\n",
    "\n",
    "```\n",
    "using CPU\n",
    "Epoch 2/20, eval_loss=1.0522257089614868\n",
    "Epoch 4/20, eval_loss=0.802649736404419\n",
    "Epoch 6/20, eval_loss=0.622248113155365\n",
    "Epoch 8/20, eval_loss=0.5638623237609863\n",
    "Epoch 10/20, eval_loss=0.488038033246994\n",
    "Epoch 12/20, eval_loss=0.45590531826019287\n",
    "Epoch 14/20, eval_loss=0.36869552731513977\n",
    "Epoch 16/20, eval_loss=0.3543365001678467\n",
    "Epoch 18/20, eval_loss=0.3283920884132385\n",
    "Epoch 20/20, eval_loss=0.32101118564605713\n",
    "Test Loss: 0.12472817301750183, Acc: 0.9769999980926514\n",
    "CPU times: user 10min 22s, sys: 921 ms, total: 10min 23s\n",
    "Wall time: 3min 32s\n",
    "```\n",
    "\n",
    "* STEM Cloud GPU-PC3 GPU: \n",
    "\n",
    "BatchNet(28*28, 300, 100, 10)  \n",
    "learning rate: 0.1; number of epocs: 20; criterion: cross entropy; optimizer: Adam\n",
    "\n",
    "```\n",
    "using GPU: _CudaDeviceProperties(name='Quadro RTX 4000', major=7, minor=5, total_memory=7979MB, multi_processor_count=36)\n",
    "Epoch 2/20, eval_loss=1.1248834133148193\n",
    "Epoch 4/20, eval_loss=0.8042405247688293\n",
    "Epoch 6/20, eval_loss=0.6801415681838989\n",
    "Epoch 8/20, eval_loss=0.5711089968681335\n",
    "Epoch 10/20, eval_loss=0.48823875188827515\n",
    "Epoch 12/20, eval_loss=0.454078733921051\n",
    "Epoch 14/20, eval_loss=0.4259450435638428\n",
    "Epoch 16/20, eval_loss=0.41063377261161804\n",
    "Epoch 18/20, eval_loss=0.3817565143108368\n",
    "Epoch 20/20, eval_loss=0.33919423818588257\n",
    "Test Loss: 0.1380709558725357, Acc: 0.9710999727249146\n",
    "CPU times: user 2min 26s, sys: 236 ms, total: 2min 26s\n",
    "Wall time: 2min 27s\n",
    "```\n",
    "\n",
    "BatchNet(28*28, 1000, 100, 10)  \n",
    "learning rate: 0.1; number of epocs: 20; criterion: cross entropy; optimizer: Adam\n",
    "\n",
    "```\n",
    "using GPU: _CudaDeviceProperties(name='Quadro RTX 4000', major=7, minor=5, total_memory=7979MB, multi_processor_count=36)\n",
    "Epoch 2/20, eval_loss=1.0551592111587524\n",
    "Epoch 4/20, eval_loss=0.7593546509742737\n",
    "Epoch 6/20, eval_loss=0.6158040165901184\n",
    "Epoch 8/20, eval_loss=0.554054856300354\n",
    "Epoch 10/20, eval_loss=0.45534154772758484\n",
    "Epoch 12/20, eval_loss=0.4187051057815552\n",
    "Epoch 14/20, eval_loss=0.39170771837234497\n",
    "Epoch 16/20, eval_loss=0.3616528809070587\n",
    "Epoch 18/20, eval_loss=0.3312235176563263\n",
    "Epoch 20/20, eval_loss=0.26851886510849\n",
    "Test Loss: 0.1363881677389145, Acc: 0.9768999814987183\n",
    "CPU times: user 2min 24s, sys: 280 ms, total: 2min 24s\n",
    "Wall time: 2min 24s\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: make prediction\n",
    "\n",
    "This step is similar to the validation step except that we are not comparing the predictions as there's no ground truth of target to compare with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size=batch_size\n",
    "# let shuffle be True to load random images each time\n",
    "sample_loader = DataLoader(test_dataset, batch_size=sample_size, shuffle=True) \n",
    "data=iter(sample_loader)\n",
    "samples,sample_labels=data.next()\n",
    "\n",
    "row_num=8 # if batch_size=64, plot matrix as 8x8\n",
    "column_num=int(batch_size/row_num)\n",
    "\n",
    "output = model(samples.view(samples.size(0),-1))\n",
    "_, pred = torch.max(output, 1)\n",
    "\n",
    "# print(sample_labels)\n",
    "# print(pred)\n",
    "\n",
    "fig, axes = plt.subplots(row_num, column_num, figsize=(8, 8),\n",
    "                         subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                         gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "\n",
    "error = 0 # prediction error\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(samples[i][0], cmap='Greys', interpolation='nearest')\n",
    "    ax.text(0.1, 0.1, sample_labels[i].numpy(), transform=ax.transAxes, color='green')\n",
    "    if sample_labels[i].numpy()==pred[i].numpy():\n",
    "        color_pred='green'\n",
    "    else:\n",
    "        color_pred='red'\n",
    "        error+=1\n",
    "    ax.text(0.8, 0.1, pred[i].numpy(), transform=ax.transAxes, color=color_pred)\n",
    "\n",
    "accuracy_predict=(1-error*1.0/sample_size)*100\n",
    "print(f'Predition Errors: {error} of {sample_size} | Prediction accuracy: {accuracy_predict}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "```Python\n",
    "model.eval()\n",
    "sample_size=batch_size # take same size sample as batch size\n",
    "sample_list=np.random.randint(len(test_loader), size=(sample_size))\n",
    "# print(f'sample list: {sample_list}')\n",
    "\n",
    "row_num=8 # if batch_size=64, plot matrix as 8x8\n",
    "column_num=int(sample_size/row_num)\n",
    "\n",
    "image_list=[test_dataset.data[i] for i in sample_list]\n",
    "label_list=[test_dataset.targets[i].numpy() for i in sample_list]\n",
    "image_tensor_list=torch.stack(tmp,0)\n",
    "image_tensor_list=image_tensor_list.view(ttmp.size(0),-1).float()#reshape and transfer to float data for learning\n",
    "\n",
    "output = model(image_tensor_list)\n",
    "_, pred = torch.max(output, 1)\n",
    "\n",
    "print(label_list)\n",
    "print(pred)\n",
    "\n",
    "#---create a grid figure----\n",
    "fig, axes = plt.subplots(row_num, column_num, figsize=(8, 8),\n",
    "                         subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                         gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "\n",
    "#---iterate and plot figures---\n",
    "error=0\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    image = image_list[i]\n",
    "    label_original = label_list[i]\n",
    "    ax.imshow(image, cmap='Greys', interpolation='nearest') # show image\n",
    "    ax.text(0.1, 0.1, label_original, transform=ax.transAxes, color='green') # show correct label\n",
    "    # if prediction is correct the color is green; o.w. the predition label is red\n",
    "    if label_original==pred[i].numpy(): \n",
    "        color_pred='green'\n",
    "    else:\n",
    "        color_pred='red'\n",
    "        error+=1\n",
    "    ax.text(0.8, 0.1, pred[i].numpy(), transform=ax.transAxes, color=color_pred)\n",
    "\n",
    "accuracy_predict=1-error*1.0/sample_size\n",
    "print(f'Prediction accuracy: {accuracy_predict}')\n",
    "```\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font style=\"font-size:30px\" color=\"red\">Your Turn</font>\n",
    "\n",
    "1. Plot the loss or accuracy with epoch using matplotlib.\n",
    "\n",
    "1. Try different values for num_epochs.\n",
    "\n",
    "1. Try different values for batch_size such as 64, 128, 256, 512.\n",
    "\n",
    "1. Try different learning rate (0.0001, 0.001, 0.1). What is your observation of the evalation loss (`eval_loss`) in training?\n",
    "\n",
    "1. `relu()` was used as the activation function for the MLP model we implemented. Try other activation functions such as `sigmoid()` and `tanh()`. What is your observation in training? (You may want to refer to the [PyTorch Functional](https://pytorch.org/docs/stable/nn.functional.html) for more details.)\n",
    "    \n",
    "1. Try different optimizers such as `SGD` and `Adagrad`.  What is your observation in training? (Refer to [PyTorch Optimizer](https://pytorch.org/docs/stable/optim.html))\n",
    "\n",
    "1. Try to add/ remove hiden layers, as well as different number of neurons and report your validation results.  What is your observation in training?\n",
    "\n",
    "1. What can you think of if we want to improve the current model?\n",
    "\n",
    "\n",
    "<!-- # Assignments: -->\n",
    "\n",
    "\n",
    "<!-- 1. Can you implment the following functions for model training, evaluation and prediction so we can reuse them when we need to test different things afterwards without having to replicate the codes every time.\n",
    "\n",
    "```python\n",
    "\n",
    "def train_model(nn_model, train_loader, optimizer, criterion, n_epoch):\n",
    "    # YOUR IMPLEMENTATION\n",
    "    return nn_model\n",
    "\n",
    "def eval_model(nn_model, val_loader):\n",
    "    # YOUR IMPLEMENTATION\n",
    "    return calculated_accuracy\n",
    "\n",
    "\n",
    "def predict(nn_model, test_loader):\n",
    "    # YOUR IMPLEMENTATION\n",
    "    return predictions\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "2. F.sigmoid() was used as the activation function for the MLP model we implemented. Can you try other activation functions such as F.relu() and F.tanh()? You may want to refer to the [PyTorch Functional](https://pytorch.org/docs/stable/nn.functional.html) for more details. Use the train_model(), eval_model() functions you implmented so you don't have to repeat the same codes.\n",
    "\n",
    "3. Can you add a dropout layer between input and the hidden layer, and another one between the hidden layer and the output layer?\n",
    "\n",
    "4. Try to add/ remove hiden layers, as well as different number of neurons and report your validation results.\n",
    "\n",
    "5. Try different learning rate (0.0001, 0.001, 0.01, 0.1). What is your observation?\n",
    "\n",
    "6. Try different values for batch_size(64, 128, 256, 512).\n",
    "\n",
    "7. Try different values for n_epochs.\n",
    "\n",
    "8. What can you think of if we want to improve the current model?\n",
    " -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (Conda 2022.05) [python/3.9-2022.05]",
   "language": "python",
   "name": "python39_202205"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
